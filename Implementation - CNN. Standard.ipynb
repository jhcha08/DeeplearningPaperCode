{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 케라스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\venv\\lib\\site-packages\\keras-2.3.1-py3.7.egg\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 421,642\n",
      "Trainable params: 421,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "inputs = Input(shape = (28,28,1))\n",
    "x = Conv2D(filters = 32, kernel_size = (3,3), strides = (1,1), padding = 'same')(inputs)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size = (2,2), strides = (2,2))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Conv2D(filters = 64, kernel_size = (3,3), strides = (1,1), padding = 'same')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = MaxPooling2D(pool_size = (2,2), strides = (2,2))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128)(x)\n",
    "outputs = Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             320\n",
      "              ReLU-2           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 32, 14, 14]               0\n",
      "            Conv2d-4           [-1, 64, 14, 14]          18,496\n",
      "              ReLU-5           [-1, 64, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 64, 7, 7]               0\n",
      "            Linear-7                  [-1, 128]         401,536\n",
      "              ReLU-8                  [-1, 128]               0\n",
      "            Linear-9                   [-1, 10]           1,290\n",
      "             ReLU-10                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 421,642\n",
      "Trainable params: 421,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.65\n",
      "Params size (MB): 1.61\n",
      "Estimated Total Size (MB): 2.26\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Standard(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Standard, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.relu1 = nn.ReLU(inplace = True) # relu를 통과한 값이 갱신되도록 해주는게 inplace=True\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.relu2 = nn.ReLU(inplace = True)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2, stride = 2) # 여기까지 왔을 때의 output이 7*7*64=3136\n",
    "        self.dense1 = nn.Linear(in_features = 3136, out_features = 128)\n",
    "        self.relu3 = nn.ReLU(inplace = True)\n",
    "        self.dense2 = nn.Linear(in_features = 128, out_features = 10)\n",
    "        self.relu4 = nn.ReLU(inplace = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.maxpool2(out)\n",
    "        out = out.view(out.size(0),-1) # view는 토치에서 reshape 기능, 그래서 Flatten으로 활용\n",
    "        out = self.dense1(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.dense2(out)\n",
    "        out = self.relu4(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from torchsummary import summary\n",
    "    model = Standard()\n",
    "    summary(model, (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.optim as optim\n",
    "\n",
    "#  GPU를 사용 가능하다면 device 값이 cuda가 되고, 아니라면 cpu\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "\n",
    "torch.manual_seed(777)\n",
    "\n",
    "# GPU 사용 가능일 경우 랜덤 시드 고정\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "# 학습에 사용할 파라미터를 설정\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# 데이터셋을 정의\n",
    "\n",
    "mnist_train = MNIST(root='MNIST_data/', # 다운로드 경로 지정\n",
    "                    train=True, # True로 지정하면 훈련 데이터로 다운로드\n",
    "                    transform=ToTensor(), # 텐서로 변환\n",
    "                    download=True)\n",
    "\n",
    "mnist_test = MNIST(root='MNIST_data/', \n",
    "                   train=False, # False로 지정하면 훈련 데이터로 다운로드\n",
    "                   transform=ToTensor(), \n",
    "                   download=True)\n",
    "\n",
    "# 데이터 로더를 사용하여 배치 크기 지정\n",
    "\n",
    "data_loader = DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# 모델 정의\n",
    "\n",
    "model = Standard().to(device)\n",
    "\n",
    "# loss function과 optimizer 정의 (크로스 엔트로피, Adam)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 총 배치 수는 600이고 배치 크기가 100이므로 훈련 데이터는 60000개\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "print('총 배치의 수 : {}'.format(total_batch))\n",
    "\n",
    "# 모델 훈련 과정\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    avg_loss = 0\n",
    "    \n",
    "    # X는 mnist_train.train_data[i] 즉 미니 배치 \n",
    "    # Y는 mnist_train.train_labels[i] 즉 레이블\n",
    "    \n",
    "    for X, Y in data_loader: \n",
    "        \n",
    "        X = X.to(device)\n",
    "        \n",
    "        y_true = Y.to(device)\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y_true)\n",
    "        \n",
    "        optimizer.zero_grad() # gradient를 0으로 초기화\n",
    "        \n",
    "        loss.backward() # loss를 역전파\n",
    "        \n",
    "        optimizer.step() # 역전파된 loss를 가지고 weights를 갱신\n",
    "        \n",
    "        avg_loss += loss / total_batch\n",
    "        \n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_loss))\n",
    "    \n",
    "# 모델 테스트 과정\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    acc = correct_prediction.float().mean()\n",
    "    print('Accuracy:', acc.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
