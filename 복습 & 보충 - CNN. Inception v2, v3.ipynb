{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d796c191",
   "metadata": {},
   "source": [
    "## 파이토치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71ad620",
   "metadata": {},
   "source": [
    "- Inception V2의 각 컨볼루션 레이어 다음에 BN을 넣고 Auxiliary classifer를 붙인 게 곧 Inception V3. 따라서 V2를 만들고 싶으면 두 기능을 주석 처리하면 된다.\n",
    "\n",
    "\n",
    "- 각 컨볼루션 다음에는 ReLU가 들어간다. Inveption v3의 경우 BN도 들어간다. 이것을 간단히 묶기 위해 ConvBlock 클래스 정의함.\n",
    "\n",
    "\n",
    "- 각 모듈의 output 사이즈는 다음 모듈의 input 사이즈다.\n",
    "\n",
    "\n",
    "- 각 인셉션 모듈 묶음 사이에는 효과적인 풀링을 하기 위한 Fig.10 인셉션 모듈(코드에서 InceptionConnect)이 들어간다. 여기서 채널 수도 다음 모듈에 맞게 늘어난다.\n",
    "\n",
    "\n",
    "- 각 인셉션 모듈 안에 있는 컨볼루션 필터들엔 제로 패딩이 들어가있다.\n",
    "\n",
    "\n",
    "- 각 인셉션 모듈 안에서 동작하는 컨볼루션 필터 개수는 따로 정해지지 않아서 임의로 설정한다. concat 대상인 아웃풋 필터 개수만 맞춘다.\n",
    "\n",
    "\n",
    "- 각 인셉션 모듈 안에서는 인풋과 아웃풋 사이즈가 같다.\n",
    "\n",
    "\n",
    "- 각 인셉션 모듈 안에서 아웃풋 쪽 제외한 그사이의 컨볼루션 통과할 때, 그 채널 수는 따로 정해지지 않아서 임의로 설정함. (ex. Fig.6의 f6 변수 같은 것)\n",
    "\n",
    "\n",
    "- 1x1 컨볼루션에선 padding을 0으로 설정해야, 즉 패딩이 없어야 stride=1로 필터가 이동해도 그 크기가 유지됨. 이전 구글넷 구조 참고.\n",
    "\n",
    "\n",
    "- Fig.6 & 7 인셉션 모듈에서는 (1xn), (nx1) 필터를 사용한다. 그에 따라 패딩도 직사각형 형태를 써줘야 한다. 코드 참고.\n",
    "\n",
    "\n",
    "- Auxiliary classifier는 17x17 레이어 다음, 즉 Fig.6 인셉션 모듈 다음에 들어간다.\n",
    "\n",
    "\n",
    "- 첫 번째 인셉션 모듈 Fig.7의 인풋은 1280이지만, 두 번째 인셉션 모듈 Fig.7 다음의 풀링 레이어에 들어가야 하는 인풋이 2048이여야 하므로, 다른 인셉션 모듈과 다르게 Fig.7은 아웃풋 사이즈가 2048이 되어야 한다.\n",
    "\n",
    "\n",
    "- 논문에 padding 등의 정보가 없으면 직접 해보면서 조금씩 summary를 해보자. 특히 Fig.10 인셉션 모듈 구현할 때 그렇게 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "795eb7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, padding):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        out = self.relu(x)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "class InceptionF5(nn.Module): # 288 = 64x3+96\n",
    "    def __init__(self, in_ch): \n",
    "        super(InceptionF5, self).__init__()\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 64, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(64, 96, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(96, 96, kernel_size=3, stride=1, padding=1)) # 96\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 48, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(48, 64, kernel_size=3, stride=1, padding=1)) # 64\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(in_ch, 64, kernel_size=3, stride=1, padding=1)) # 64\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 64, kernel_size=1, stride=1, padding=0)) # 64\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x4 = self.branch4(x)\n",
    "        \n",
    "        return torch.cat([x1, x2, x3, x4], dim=1)\n",
    "    \n",
    "    \n",
    "class InceptionF6(nn.Module): # 768 = 192x4\n",
    "    def __init__(self, in_ch, f6): \n",
    "        super(InceptionF6, self).__init__()\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvBlock(in_ch, f6, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(f6, f6, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            ConvBlock(f6, f6, kernel_size=(7,1), stride=1, padding=(3,0)),\n",
    "            ConvBlock(f6, f6, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            ConvBlock(f6, 192, kernel_size=(7,1), stride=1, padding=(3,0))) # 192\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_ch, f6, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(f6, f6, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            ConvBlock(f6, 192, kernel_size=(7,1), stride=1, padding=(3,0))) # 192\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(in_ch, 192, kernel_size=3, stride=1, padding=1)) # 192\n",
    "        \n",
    "        self.branch4 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 192, kernel_size=1, stride=1, padding=0)) # 192\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x4 = self.branch4(x)\n",
    "        \n",
    "        return torch.cat([x1, x2, x3, x4], dim=1)\n",
    "    \n",
    "    \n",
    "class InceptionF7(nn.Module): # 2048 = 384x4+192+320\n",
    "    def __init__(self, in_ch): \n",
    "        super(InceptionF7, self).__init__()\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 448, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(448, 384, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        self.branch1_1 = ConvBlock(384, 384, kernel_size=(1,3), stride=1, padding=(0,1))\n",
    "        self.branch1_2 = ConvBlock(384, 384, kernel_size=(3,1), stride=1, padding=(1,0)) # 384\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 384, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "        self.branch2_1 = ConvBlock(384, 384, kernel_size=(1,3), stride=1, padding=(0,1))\n",
    "        self.branch2_2 = ConvBlock(384, 384, kernel_size=(3,1), stride=1, padding=(1,0)) # 384\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(in_ch, 192, kernel_size=1, stride=1, padding=0)) # 192\n",
    "        \n",
    "        self.branch4 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 320, kernel_size=1, stride=1, padding=0)) # 320\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.branch1(x)\n",
    "        x1 = torch.cat([self.branch1_1(x1), self.branch1_2(x1)], dim=1)\n",
    "        \n",
    "        x2 = self.branch2(x)\n",
    "        x2 = torch.cat([self.branch2_1(x2), self.branch2_2(x2)], dim=1)\n",
    "        \n",
    "        x3 = self.branch3(x)\n",
    "        x4 = self.branch4(x)\n",
    "        \n",
    "        return torch.cat([x1, x2, x3, x4], dim=1)\n",
    "    \n",
    "    \n",
    "class InceptionConnect(nn.Module):\n",
    "    def __init__(self, in_ch, add_ch=0):           # Fig.5와 Fig.6를 연결하려면 아웃풋 768 = 240+240+288\n",
    "        super(InceptionConnect, self).__init__()   # Fig.6와 Fig.7를 연결하려면 아웃풋 1280 = 256+256+768\n",
    "        \n",
    "         # Fig.5 & Fig.6 -> 240, Fig.6 & Fig.7 -> 256 = 240+16\n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 64+add_ch, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(64+add_ch, 96+add_ch, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(96+add_ch, 240+add_ch, kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        # Fig.5 & Fig.6 -> 240, Fig.6 & Fig.7 -> 256 = 240+16\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 48+add_ch, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(48+add_ch, 240+add_ch, kernel_size=3, stride=2, padding=0)) \n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0)) # 풀링 결과(풀링 레이어에선 채널 수 안 바뀌므로 인풋 채널과 같음)\n",
    "                                                              # Fig.5 & Fig.6 -> 288, Fig.6 & Fig.7 -> 768\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        \n",
    "        out = torch.cat([x1, x2, x3], dim=1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class InceptionAux(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        \n",
    "        self.pool = nn.AvgPool2d(5, stride=3, padding=0)\n",
    "        self.conv = nn.Conv2d(768, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(3200, 1024)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        aux = self.linear(x)\n",
    "        \n",
    "        return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de221074",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV2(nn.Module):\n",
    "    def __init__(self, num_classes = 1000):\n",
    "        super(InceptionV2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 80, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv5 = nn.Conv2d(80, 192, kernel_size=3, stride=2, padding=0)\n",
    "        self.conv6 = nn.Conv2d(192, 288, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.inceptionF5_1 = InceptionF5(288)\n",
    "        self.inceptionF5_2 = InceptionF5(288)\n",
    "        self.inceptionF5_3 = InceptionF5(288)\n",
    "        \n",
    "        self.inception_connect1 = InceptionConnect(288, add_ch=0)\n",
    "        \n",
    "        self.inceptionF6_1 = InceptionF6(768, f6=128)\n",
    "        self.inceptionF6_2 = InceptionF6(768, f6=160)\n",
    "        self.inceptionF6_3 = InceptionF6(768, f6=160)\n",
    "        self.inceptionF6_4 = InceptionF6(768, f6=160)\n",
    "        self.inceptionF6_5 = InceptionF6(768, f6=160)\n",
    "        self.inceptionF6_6 = InceptionF6(768, f6=192)\n",
    "        \n",
    "        self.auxiliary = InceptionAux(768)\n",
    "        \n",
    "        self.inception_connect2 = InceptionConnect(768, add_ch=16)\n",
    "        \n",
    "        self.inceptionF7_1 = InceptionF7(1280)\n",
    "        self.inceptionF7_2 = InceptionF7(2048)\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=8, stride=1, padding=0)\n",
    "        \n",
    "        self.linear = nn.Linear(2048, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        \n",
    "        x = self.inceptionF5_1(x)\n",
    "        x = self.inceptionF5_2(x)\n",
    "        x = self.inceptionF5_3(x)\n",
    "        \n",
    "        x = self.inception_connect1(x)\n",
    "        \n",
    "        x = self.inceptionF6_1(x)\n",
    "        x = self.inceptionF6_2(x)\n",
    "        x = self.inceptionF6_3(x)\n",
    "        x = self.inceptionF6_4(x)\n",
    "        x = self.inceptionF6_5(x)\n",
    "        x = self.inceptionF6_6(x)\n",
    "        \n",
    "        aux = self.auxiliary(x)\n",
    "        \n",
    "        x = self.inception_connect2(x)\n",
    "        \n",
    "        x = self.inceptionF7_1(x)\n",
    "        x = self.inceptionF7_2(x)\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0),-1)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return aux, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f3d5f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 149, 149]             896\n",
      "            Conv2d-2         [-1, 32, 147, 147]           9,248\n",
      "            Conv2d-3         [-1, 64, 147, 147]          18,496\n",
      "         MaxPool2d-4           [-1, 64, 73, 73]               0\n",
      "            Conv2d-5           [-1, 80, 71, 71]          46,160\n",
      "            Conv2d-6          [-1, 192, 35, 35]         138,432\n",
      "            Conv2d-7          [-1, 288, 35, 35]          55,584\n",
      "            Conv2d-8           [-1, 64, 35, 35]          18,432\n",
      "       BatchNorm2d-9           [-1, 64, 35, 35]             128\n",
      "             ReLU-10           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-11           [-1, 64, 35, 35]               0\n",
      "           Conv2d-12           [-1, 96, 35, 35]          55,296\n",
      "      BatchNorm2d-13           [-1, 96, 35, 35]             192\n",
      "             ReLU-14           [-1, 96, 35, 35]               0\n",
      "        ConvBlock-15           [-1, 96, 35, 35]               0\n",
      "           Conv2d-16           [-1, 96, 35, 35]          82,944\n",
      "      BatchNorm2d-17           [-1, 96, 35, 35]             192\n",
      "             ReLU-18           [-1, 96, 35, 35]               0\n",
      "        ConvBlock-19           [-1, 96, 35, 35]               0\n",
      "           Conv2d-20           [-1, 48, 35, 35]          13,824\n",
      "      BatchNorm2d-21           [-1, 48, 35, 35]              96\n",
      "             ReLU-22           [-1, 48, 35, 35]               0\n",
      "        ConvBlock-23           [-1, 48, 35, 35]               0\n",
      "           Conv2d-24           [-1, 64, 35, 35]          27,648\n",
      "      BatchNorm2d-25           [-1, 64, 35, 35]             128\n",
      "             ReLU-26           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-27           [-1, 64, 35, 35]               0\n",
      "        MaxPool2d-28          [-1, 288, 35, 35]               0\n",
      "           Conv2d-29           [-1, 64, 35, 35]         165,888\n",
      "      BatchNorm2d-30           [-1, 64, 35, 35]             128\n",
      "             ReLU-31           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-32           [-1, 64, 35, 35]               0\n",
      "           Conv2d-33           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-34           [-1, 64, 35, 35]             128\n",
      "             ReLU-35           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-36           [-1, 64, 35, 35]               0\n",
      "      InceptionF5-37          [-1, 288, 35, 35]               0\n",
      "           Conv2d-38           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-39           [-1, 64, 35, 35]             128\n",
      "             ReLU-40           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-41           [-1, 64, 35, 35]               0\n",
      "           Conv2d-42           [-1, 96, 35, 35]          55,296\n",
      "      BatchNorm2d-43           [-1, 96, 35, 35]             192\n",
      "             ReLU-44           [-1, 96, 35, 35]               0\n",
      "        ConvBlock-45           [-1, 96, 35, 35]               0\n",
      "           Conv2d-46           [-1, 96, 35, 35]          82,944\n",
      "      BatchNorm2d-47           [-1, 96, 35, 35]             192\n",
      "             ReLU-48           [-1, 96, 35, 35]               0\n",
      "        ConvBlock-49           [-1, 96, 35, 35]               0\n",
      "           Conv2d-50           [-1, 48, 35, 35]          13,824\n",
      "      BatchNorm2d-51           [-1, 48, 35, 35]              96\n",
      "             ReLU-52           [-1, 48, 35, 35]               0\n",
      "        ConvBlock-53           [-1, 48, 35, 35]               0\n",
      "           Conv2d-54           [-1, 64, 35, 35]          27,648\n",
      "      BatchNorm2d-55           [-1, 64, 35, 35]             128\n",
      "             ReLU-56           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-57           [-1, 64, 35, 35]               0\n",
      "        MaxPool2d-58          [-1, 288, 35, 35]               0\n",
      "           Conv2d-59           [-1, 64, 35, 35]         165,888\n",
      "      BatchNorm2d-60           [-1, 64, 35, 35]             128\n",
      "             ReLU-61           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-62           [-1, 64, 35, 35]               0\n",
      "           Conv2d-63           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-64           [-1, 64, 35, 35]             128\n",
      "             ReLU-65           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-66           [-1, 64, 35, 35]               0\n",
      "      InceptionF5-67          [-1, 288, 35, 35]               0\n",
      "           Conv2d-68           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-69           [-1, 64, 35, 35]             128\n",
      "             ReLU-70           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-71           [-1, 64, 35, 35]               0\n",
      "           Conv2d-72           [-1, 96, 35, 35]          55,296\n",
      "      BatchNorm2d-73           [-1, 96, 35, 35]             192\n",
      "             ReLU-74           [-1, 96, 35, 35]               0\n",
      "        ConvBlock-75           [-1, 96, 35, 35]               0\n",
      "           Conv2d-76           [-1, 96, 35, 35]          82,944\n",
      "      BatchNorm2d-77           [-1, 96, 35, 35]             192\n",
      "             ReLU-78           [-1, 96, 35, 35]               0\n",
      "        ConvBlock-79           [-1, 96, 35, 35]               0\n",
      "           Conv2d-80           [-1, 48, 35, 35]          13,824\n",
      "      BatchNorm2d-81           [-1, 48, 35, 35]              96\n",
      "             ReLU-82           [-1, 48, 35, 35]               0\n",
      "        ConvBlock-83           [-1, 48, 35, 35]               0\n",
      "           Conv2d-84           [-1, 64, 35, 35]          27,648\n",
      "      BatchNorm2d-85           [-1, 64, 35, 35]             128\n",
      "             ReLU-86           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-87           [-1, 64, 35, 35]               0\n",
      "        MaxPool2d-88          [-1, 288, 35, 35]               0\n",
      "           Conv2d-89           [-1, 64, 35, 35]         165,888\n",
      "      BatchNorm2d-90           [-1, 64, 35, 35]             128\n",
      "             ReLU-91           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-92           [-1, 64, 35, 35]               0\n",
      "           Conv2d-93           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-94           [-1, 64, 35, 35]             128\n",
      "             ReLU-95           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-96           [-1, 64, 35, 35]               0\n",
      "      InceptionF5-97          [-1, 288, 35, 35]               0\n",
      "           Conv2d-98           [-1, 64, 35, 35]          18,432\n",
      "      BatchNorm2d-99           [-1, 64, 35, 35]             128\n",
      "            ReLU-100           [-1, 64, 35, 35]               0\n",
      "       ConvBlock-101           [-1, 64, 35, 35]               0\n",
      "          Conv2d-102           [-1, 96, 35, 35]          55,296\n",
      "     BatchNorm2d-103           [-1, 96, 35, 35]             192\n",
      "            ReLU-104           [-1, 96, 35, 35]               0\n",
      "       ConvBlock-105           [-1, 96, 35, 35]               0\n",
      "          Conv2d-106          [-1, 240, 17, 17]         207,360\n",
      "     BatchNorm2d-107          [-1, 240, 17, 17]             480\n",
      "            ReLU-108          [-1, 240, 17, 17]               0\n",
      "       ConvBlock-109          [-1, 240, 17, 17]               0\n",
      "          Conv2d-110           [-1, 48, 35, 35]          13,824\n",
      "     BatchNorm2d-111           [-1, 48, 35, 35]              96\n",
      "            ReLU-112           [-1, 48, 35, 35]               0\n",
      "       ConvBlock-113           [-1, 48, 35, 35]               0\n",
      "          Conv2d-114          [-1, 240, 17, 17]         103,680\n",
      "     BatchNorm2d-115          [-1, 240, 17, 17]             480\n",
      "            ReLU-116          [-1, 240, 17, 17]               0\n",
      "       ConvBlock-117          [-1, 240, 17, 17]               0\n",
      "       MaxPool2d-118          [-1, 288, 17, 17]               0\n",
      "InceptionConnect-119          [-1, 768, 17, 17]               0\n",
      "          Conv2d-120          [-1, 128, 17, 17]          98,304\n",
      "     BatchNorm2d-121          [-1, 128, 17, 17]             256\n",
      "            ReLU-122          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-123          [-1, 128, 17, 17]               0\n",
      "          Conv2d-124          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-125          [-1, 128, 17, 17]             256\n",
      "            ReLU-126          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-127          [-1, 128, 17, 17]               0\n",
      "          Conv2d-128          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-129          [-1, 128, 17, 17]             256\n",
      "            ReLU-130          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-131          [-1, 128, 17, 17]               0\n",
      "          Conv2d-132          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-133          [-1, 128, 17, 17]             256\n",
      "            ReLU-134          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-135          [-1, 128, 17, 17]               0\n",
      "          Conv2d-136          [-1, 192, 17, 17]         172,032\n",
      "     BatchNorm2d-137          [-1, 192, 17, 17]             384\n",
      "            ReLU-138          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-139          [-1, 192, 17, 17]               0\n",
      "          Conv2d-140          [-1, 128, 17, 17]          98,304\n",
      "     BatchNorm2d-141          [-1, 128, 17, 17]             256\n",
      "            ReLU-142          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-143          [-1, 128, 17, 17]               0\n",
      "          Conv2d-144          [-1, 128, 17, 17]         114,688\n",
      "     BatchNorm2d-145          [-1, 128, 17, 17]             256\n",
      "            ReLU-146          [-1, 128, 17, 17]               0\n",
      "       ConvBlock-147          [-1, 128, 17, 17]               0\n",
      "          Conv2d-148          [-1, 192, 17, 17]         172,032\n",
      "     BatchNorm2d-149          [-1, 192, 17, 17]             384\n",
      "            ReLU-150          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-151          [-1, 192, 17, 17]               0\n",
      "       MaxPool2d-152          [-1, 768, 17, 17]               0\n",
      "          Conv2d-153          [-1, 192, 17, 17]       1,327,104\n",
      "     BatchNorm2d-154          [-1, 192, 17, 17]             384\n",
      "            ReLU-155          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-156          [-1, 192, 17, 17]               0\n",
      "          Conv2d-157          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-158          [-1, 192, 17, 17]             384\n",
      "            ReLU-159          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-160          [-1, 192, 17, 17]               0\n",
      "     InceptionF6-161          [-1, 768, 17, 17]               0\n",
      "          Conv2d-162          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-163          [-1, 160, 17, 17]             320\n",
      "            ReLU-164          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-165          [-1, 160, 17, 17]               0\n",
      "          Conv2d-166          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-167          [-1, 160, 17, 17]             320\n",
      "            ReLU-168          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-169          [-1, 160, 17, 17]               0\n",
      "          Conv2d-170          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-171          [-1, 160, 17, 17]             320\n",
      "            ReLU-172          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-173          [-1, 160, 17, 17]               0\n",
      "          Conv2d-174          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-175          [-1, 160, 17, 17]             320\n",
      "            ReLU-176          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-177          [-1, 160, 17, 17]               0\n",
      "          Conv2d-178          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-179          [-1, 192, 17, 17]             384\n",
      "            ReLU-180          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-181          [-1, 192, 17, 17]               0\n",
      "          Conv2d-182          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-183          [-1, 160, 17, 17]             320\n",
      "            ReLU-184          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-185          [-1, 160, 17, 17]               0\n",
      "          Conv2d-186          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-187          [-1, 160, 17, 17]             320\n",
      "            ReLU-188          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-189          [-1, 160, 17, 17]               0\n",
      "          Conv2d-190          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-191          [-1, 192, 17, 17]             384\n",
      "            ReLU-192          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-193          [-1, 192, 17, 17]               0\n",
      "       MaxPool2d-194          [-1, 768, 17, 17]               0\n",
      "          Conv2d-195          [-1, 192, 17, 17]       1,327,104\n",
      "     BatchNorm2d-196          [-1, 192, 17, 17]             384\n",
      "            ReLU-197          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-198          [-1, 192, 17, 17]               0\n",
      "          Conv2d-199          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-200          [-1, 192, 17, 17]             384\n",
      "            ReLU-201          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-202          [-1, 192, 17, 17]               0\n",
      "     InceptionF6-203          [-1, 768, 17, 17]               0\n",
      "          Conv2d-204          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-205          [-1, 160, 17, 17]             320\n",
      "            ReLU-206          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-207          [-1, 160, 17, 17]               0\n",
      "          Conv2d-208          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-209          [-1, 160, 17, 17]             320\n",
      "            ReLU-210          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-211          [-1, 160, 17, 17]               0\n",
      "          Conv2d-212          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-213          [-1, 160, 17, 17]             320\n",
      "            ReLU-214          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-215          [-1, 160, 17, 17]               0\n",
      "          Conv2d-216          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-217          [-1, 160, 17, 17]             320\n",
      "            ReLU-218          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-219          [-1, 160, 17, 17]               0\n",
      "          Conv2d-220          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-221          [-1, 192, 17, 17]             384\n",
      "            ReLU-222          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-223          [-1, 192, 17, 17]               0\n",
      "          Conv2d-224          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-225          [-1, 160, 17, 17]             320\n",
      "            ReLU-226          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-227          [-1, 160, 17, 17]               0\n",
      "          Conv2d-228          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-229          [-1, 160, 17, 17]             320\n",
      "            ReLU-230          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-231          [-1, 160, 17, 17]               0\n",
      "          Conv2d-232          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-233          [-1, 192, 17, 17]             384\n",
      "            ReLU-234          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-235          [-1, 192, 17, 17]               0\n",
      "       MaxPool2d-236          [-1, 768, 17, 17]               0\n",
      "          Conv2d-237          [-1, 192, 17, 17]       1,327,104\n",
      "     BatchNorm2d-238          [-1, 192, 17, 17]             384\n",
      "            ReLU-239          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-240          [-1, 192, 17, 17]               0\n",
      "          Conv2d-241          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-242          [-1, 192, 17, 17]             384\n",
      "            ReLU-243          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-244          [-1, 192, 17, 17]               0\n",
      "     InceptionF6-245          [-1, 768, 17, 17]               0\n",
      "          Conv2d-246          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-247          [-1, 160, 17, 17]             320\n",
      "            ReLU-248          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-249          [-1, 160, 17, 17]               0\n",
      "          Conv2d-250          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-251          [-1, 160, 17, 17]             320\n",
      "            ReLU-252          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-253          [-1, 160, 17, 17]               0\n",
      "          Conv2d-254          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-255          [-1, 160, 17, 17]             320\n",
      "            ReLU-256          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-257          [-1, 160, 17, 17]               0\n",
      "          Conv2d-258          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-259          [-1, 160, 17, 17]             320\n",
      "            ReLU-260          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-261          [-1, 160, 17, 17]               0\n",
      "          Conv2d-262          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-263          [-1, 192, 17, 17]             384\n",
      "            ReLU-264          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-265          [-1, 192, 17, 17]               0\n",
      "          Conv2d-266          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-267          [-1, 160, 17, 17]             320\n",
      "            ReLU-268          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-269          [-1, 160, 17, 17]               0\n",
      "          Conv2d-270          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-271          [-1, 160, 17, 17]             320\n",
      "            ReLU-272          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-273          [-1, 160, 17, 17]               0\n",
      "          Conv2d-274          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-275          [-1, 192, 17, 17]             384\n",
      "            ReLU-276          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-277          [-1, 192, 17, 17]               0\n",
      "       MaxPool2d-278          [-1, 768, 17, 17]               0\n",
      "          Conv2d-279          [-1, 192, 17, 17]       1,327,104\n",
      "     BatchNorm2d-280          [-1, 192, 17, 17]             384\n",
      "            ReLU-281          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-282          [-1, 192, 17, 17]               0\n",
      "          Conv2d-283          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-284          [-1, 192, 17, 17]             384\n",
      "            ReLU-285          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-286          [-1, 192, 17, 17]               0\n",
      "     InceptionF6-287          [-1, 768, 17, 17]               0\n",
      "          Conv2d-288          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-289          [-1, 160, 17, 17]             320\n",
      "            ReLU-290          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-291          [-1, 160, 17, 17]               0\n",
      "          Conv2d-292          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-293          [-1, 160, 17, 17]             320\n",
      "            ReLU-294          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-295          [-1, 160, 17, 17]               0\n",
      "          Conv2d-296          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-297          [-1, 160, 17, 17]             320\n",
      "            ReLU-298          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-299          [-1, 160, 17, 17]               0\n",
      "          Conv2d-300          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-301          [-1, 160, 17, 17]             320\n",
      "            ReLU-302          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-303          [-1, 160, 17, 17]               0\n",
      "          Conv2d-304          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-305          [-1, 192, 17, 17]             384\n",
      "            ReLU-306          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-307          [-1, 192, 17, 17]               0\n",
      "          Conv2d-308          [-1, 160, 17, 17]         122,880\n",
      "     BatchNorm2d-309          [-1, 160, 17, 17]             320\n",
      "            ReLU-310          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-311          [-1, 160, 17, 17]               0\n",
      "          Conv2d-312          [-1, 160, 17, 17]         179,200\n",
      "     BatchNorm2d-313          [-1, 160, 17, 17]             320\n",
      "            ReLU-314          [-1, 160, 17, 17]               0\n",
      "       ConvBlock-315          [-1, 160, 17, 17]               0\n",
      "          Conv2d-316          [-1, 192, 17, 17]         215,040\n",
      "     BatchNorm2d-317          [-1, 192, 17, 17]             384\n",
      "            ReLU-318          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-319          [-1, 192, 17, 17]               0\n",
      "       MaxPool2d-320          [-1, 768, 17, 17]               0\n",
      "          Conv2d-321          [-1, 192, 17, 17]       1,327,104\n",
      "     BatchNorm2d-322          [-1, 192, 17, 17]             384\n",
      "            ReLU-323          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-324          [-1, 192, 17, 17]               0\n",
      "          Conv2d-325          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-326          [-1, 192, 17, 17]             384\n",
      "            ReLU-327          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-328          [-1, 192, 17, 17]               0\n",
      "     InceptionF6-329          [-1, 768, 17, 17]               0\n",
      "          Conv2d-330          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-331          [-1, 192, 17, 17]             384\n",
      "            ReLU-332          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-333          [-1, 192, 17, 17]               0\n",
      "          Conv2d-334          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-335          [-1, 192, 17, 17]             384\n",
      "            ReLU-336          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-337          [-1, 192, 17, 17]               0\n",
      "          Conv2d-338          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-339          [-1, 192, 17, 17]             384\n",
      "            ReLU-340          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-341          [-1, 192, 17, 17]               0\n",
      "          Conv2d-342          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-343          [-1, 192, 17, 17]             384\n",
      "            ReLU-344          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-345          [-1, 192, 17, 17]               0\n",
      "          Conv2d-346          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-347          [-1, 192, 17, 17]             384\n",
      "            ReLU-348          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-349          [-1, 192, 17, 17]               0\n",
      "          Conv2d-350          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-351          [-1, 192, 17, 17]             384\n",
      "            ReLU-352          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-353          [-1, 192, 17, 17]               0\n",
      "          Conv2d-354          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-355          [-1, 192, 17, 17]             384\n",
      "            ReLU-356          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-357          [-1, 192, 17, 17]               0\n",
      "          Conv2d-358          [-1, 192, 17, 17]         258,048\n",
      "     BatchNorm2d-359          [-1, 192, 17, 17]             384\n",
      "            ReLU-360          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-361          [-1, 192, 17, 17]               0\n",
      "       MaxPool2d-362          [-1, 768, 17, 17]               0\n",
      "          Conv2d-363          [-1, 192, 17, 17]       1,327,104\n",
      "     BatchNorm2d-364          [-1, 192, 17, 17]             384\n",
      "            ReLU-365          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-366          [-1, 192, 17, 17]               0\n",
      "          Conv2d-367          [-1, 192, 17, 17]         147,456\n",
      "     BatchNorm2d-368          [-1, 192, 17, 17]             384\n",
      "            ReLU-369          [-1, 192, 17, 17]               0\n",
      "       ConvBlock-370          [-1, 192, 17, 17]               0\n",
      "     InceptionF6-371          [-1, 768, 17, 17]               0\n",
      "       AvgPool2d-372            [-1, 768, 5, 5]               0\n",
      "          Conv2d-373            [-1, 128, 5, 5]          98,432\n",
      "            ReLU-374            [-1, 128, 5, 5]               0\n",
      "          Linear-375                 [-1, 1024]       3,277,824\n",
      "    InceptionAux-376                 [-1, 1024]               0\n",
      "          Conv2d-377           [-1, 80, 17, 17]          61,440\n",
      "     BatchNorm2d-378           [-1, 80, 17, 17]             160\n",
      "            ReLU-379           [-1, 80, 17, 17]               0\n",
      "       ConvBlock-380           [-1, 80, 17, 17]               0\n",
      "          Conv2d-381          [-1, 112, 17, 17]          80,640\n",
      "     BatchNorm2d-382          [-1, 112, 17, 17]             224\n",
      "            ReLU-383          [-1, 112, 17, 17]               0\n",
      "       ConvBlock-384          [-1, 112, 17, 17]               0\n",
      "          Conv2d-385            [-1, 256, 8, 8]         258,048\n",
      "     BatchNorm2d-386            [-1, 256, 8, 8]             512\n",
      "            ReLU-387            [-1, 256, 8, 8]               0\n",
      "       ConvBlock-388            [-1, 256, 8, 8]               0\n",
      "          Conv2d-389           [-1, 64, 17, 17]          49,152\n",
      "     BatchNorm2d-390           [-1, 64, 17, 17]             128\n",
      "            ReLU-391           [-1, 64, 17, 17]               0\n",
      "       ConvBlock-392           [-1, 64, 17, 17]               0\n",
      "          Conv2d-393            [-1, 256, 8, 8]         147,456\n",
      "     BatchNorm2d-394            [-1, 256, 8, 8]             512\n",
      "            ReLU-395            [-1, 256, 8, 8]               0\n",
      "       ConvBlock-396            [-1, 256, 8, 8]               0\n",
      "       MaxPool2d-397            [-1, 768, 8, 8]               0\n",
      "InceptionConnect-398           [-1, 1280, 8, 8]               0\n",
      "          Conv2d-399            [-1, 448, 8, 8]         573,440\n",
      "     BatchNorm2d-400            [-1, 448, 8, 8]             896\n",
      "            ReLU-401            [-1, 448, 8, 8]               0\n",
      "       ConvBlock-402            [-1, 448, 8, 8]               0\n",
      "          Conv2d-403            [-1, 384, 8, 8]       1,548,288\n",
      "     BatchNorm2d-404            [-1, 384, 8, 8]             768\n",
      "            ReLU-405            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-406            [-1, 384, 8, 8]               0\n",
      "          Conv2d-407            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-408            [-1, 384, 8, 8]             768\n",
      "            ReLU-409            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-410            [-1, 384, 8, 8]               0\n",
      "          Conv2d-411            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-412            [-1, 384, 8, 8]             768\n",
      "            ReLU-413            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-414            [-1, 384, 8, 8]               0\n",
      "          Conv2d-415            [-1, 384, 8, 8]         491,520\n",
      "     BatchNorm2d-416            [-1, 384, 8, 8]             768\n",
      "            ReLU-417            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-418            [-1, 384, 8, 8]               0\n",
      "          Conv2d-419            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-420            [-1, 384, 8, 8]             768\n",
      "            ReLU-421            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-422            [-1, 384, 8, 8]               0\n",
      "          Conv2d-423            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-424            [-1, 384, 8, 8]             768\n",
      "            ReLU-425            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-426            [-1, 384, 8, 8]               0\n",
      "       MaxPool2d-427           [-1, 1280, 8, 8]               0\n",
      "          Conv2d-428            [-1, 192, 8, 8]         245,760\n",
      "     BatchNorm2d-429            [-1, 192, 8, 8]             384\n",
      "            ReLU-430            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-431            [-1, 192, 8, 8]               0\n",
      "          Conv2d-432            [-1, 320, 8, 8]         409,600\n",
      "     BatchNorm2d-433            [-1, 320, 8, 8]             640\n",
      "            ReLU-434            [-1, 320, 8, 8]               0\n",
      "       ConvBlock-435            [-1, 320, 8, 8]               0\n",
      "     InceptionF7-436           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-437            [-1, 448, 8, 8]         917,504\n",
      "     BatchNorm2d-438            [-1, 448, 8, 8]             896\n",
      "            ReLU-439            [-1, 448, 8, 8]               0\n",
      "       ConvBlock-440            [-1, 448, 8, 8]               0\n",
      "          Conv2d-441            [-1, 384, 8, 8]       1,548,288\n",
      "     BatchNorm2d-442            [-1, 384, 8, 8]             768\n",
      "            ReLU-443            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-444            [-1, 384, 8, 8]               0\n",
      "          Conv2d-445            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-446            [-1, 384, 8, 8]             768\n",
      "            ReLU-447            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-448            [-1, 384, 8, 8]               0\n",
      "          Conv2d-449            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-450            [-1, 384, 8, 8]             768\n",
      "            ReLU-451            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-452            [-1, 384, 8, 8]               0\n",
      "          Conv2d-453            [-1, 384, 8, 8]         786,432\n",
      "     BatchNorm2d-454            [-1, 384, 8, 8]             768\n",
      "            ReLU-455            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-456            [-1, 384, 8, 8]               0\n",
      "          Conv2d-457            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-458            [-1, 384, 8, 8]             768\n",
      "            ReLU-459            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-460            [-1, 384, 8, 8]               0\n",
      "          Conv2d-461            [-1, 384, 8, 8]         442,368\n",
      "     BatchNorm2d-462            [-1, 384, 8, 8]             768\n",
      "            ReLU-463            [-1, 384, 8, 8]               0\n",
      "       ConvBlock-464            [-1, 384, 8, 8]               0\n",
      "       MaxPool2d-465           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-466            [-1, 192, 8, 8]         393,216\n",
      "     BatchNorm2d-467            [-1, 192, 8, 8]             384\n",
      "            ReLU-468            [-1, 192, 8, 8]               0\n",
      "       ConvBlock-469            [-1, 192, 8, 8]               0\n",
      "          Conv2d-470            [-1, 320, 8, 8]         655,360\n",
      "     BatchNorm2d-471            [-1, 320, 8, 8]             640\n",
      "            ReLU-472            [-1, 320, 8, 8]               0\n",
      "       ConvBlock-473            [-1, 320, 8, 8]               0\n",
      "     InceptionF7-474           [-1, 2048, 8, 8]               0\n",
      "       MaxPool2d-475           [-1, 2048, 1, 1]               0\n",
      "          Linear-476                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 36,245,368\n",
      "Trainable params: 36,245,368\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.02\n",
      "Forward/backward pass size (MB): 250.16\n",
      "Params size (MB): 138.27\n",
      "Estimated Total Size (MB): 389.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    from torchsummary import summary\n",
    "    model = InceptionV2()\n",
    "    summary(model, (3,299,299))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261ce8f",
   "metadata": {},
   "source": [
    "## 컨볼루션 계산 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "db26024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig.5와 Fig.6를 연결하려면 아웃풋 768 = 240+240+288\n",
    "# Fig.6와 Fig.7를 연결하려면 아웃풋 1280 = 256+256+768\n",
    "\n",
    "class InceptionF10(nn.Module):\n",
    "    def __init__(self, in_ch, add_ch=0): \n",
    "        super(InceptionF10, self).__init__()\n",
    "        \n",
    "         # Fig.5 & Fig.6 -> 240, Fig.6 & Fig.7 -> 256 = 240+16\n",
    "        self.branch1 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 64+add_ch, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(64+add_ch, 96+add_ch, kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(96+add_ch, 240+add_ch, kernel_size=3, stride=2, padding=0))\n",
    "        \n",
    "        # Fig.5 & Fig.6 -> 240, Fig.6 & Fig.7 -> 256 = 240+16\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_ch, 48+add_ch, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(48+add_ch, 240+add_ch, kernel_size=3, stride=2, padding=0)) \n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=0)) # 풀링 결과(인풋 사이즈와 같음): \n",
    "                                                              # Fig.5 & Fig.6 -> 288, Fig.6 & Fig.7 -> 768\n",
    "        \n",
    "        # 마지막 concat이 1280으로 잘 되었는지 summary로 확인하기 위함. 이걸 안하면 summary해도 최종 결과가 안보임.\n",
    "        # 아웃풋은 임의로 500\n",
    "        self.conv = ConvBlock(1280, 500, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        \n",
    "        out = torch.cat([x1, x2, x3], dim=1)\n",
    "        \n",
    "        out = self.conv(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "377b300e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 80, 35, 35]          61,440\n",
      "       BatchNorm2d-2           [-1, 80, 35, 35]             160\n",
      "              ReLU-3           [-1, 80, 35, 35]               0\n",
      "         ConvBlock-4           [-1, 80, 35, 35]               0\n",
      "            Conv2d-5          [-1, 112, 35, 35]          80,640\n",
      "       BatchNorm2d-6          [-1, 112, 35, 35]             224\n",
      "              ReLU-7          [-1, 112, 35, 35]               0\n",
      "         ConvBlock-8          [-1, 112, 35, 35]               0\n",
      "            Conv2d-9          [-1, 256, 17, 17]         258,048\n",
      "      BatchNorm2d-10          [-1, 256, 17, 17]             512\n",
      "             ReLU-11          [-1, 256, 17, 17]               0\n",
      "        ConvBlock-12          [-1, 256, 17, 17]               0\n",
      "           Conv2d-13           [-1, 64, 35, 35]          49,152\n",
      "      BatchNorm2d-14           [-1, 64, 35, 35]             128\n",
      "             ReLU-15           [-1, 64, 35, 35]               0\n",
      "        ConvBlock-16           [-1, 64, 35, 35]               0\n",
      "           Conv2d-17          [-1, 256, 17, 17]         147,456\n",
      "      BatchNorm2d-18          [-1, 256, 17, 17]             512\n",
      "             ReLU-19          [-1, 256, 17, 17]               0\n",
      "        ConvBlock-20          [-1, 256, 17, 17]               0\n",
      "        MaxPool2d-21          [-1, 768, 17, 17]               0\n",
      "           Conv2d-22          [-1, 500, 17, 17]         640,000\n",
      "      BatchNorm2d-23          [-1, 500, 17, 17]           1,000\n",
      "             ReLU-24          [-1, 500, 17, 17]               0\n",
      "        ConvBlock-25          [-1, 500, 17, 17]               0\n",
      "================================================================\n",
      "Total params: 1,239,272\n",
      "Trainable params: 1,239,272\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.59\n",
      "Forward/backward pass size (MB): 20.19\n",
      "Params size (MB): 4.73\n",
      "Estimated Total Size (MB): 28.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    from torchsummary import summary\n",
    "    model = InceptionF10(768,16)\n",
    "    summary(model, (768,35,35))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
