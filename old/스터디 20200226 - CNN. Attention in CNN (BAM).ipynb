{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             320\n",
      "              ReLU-2           [-1, 32, 28, 28]               0\n",
      " AdaptiveAvgPool2d-3             [-1, 32, 1, 1]               0\n",
      "              View-4                   [-1, 32]               0\n",
      "            Linear-5                    [-1, 2]              64\n",
      "       BatchNorm1d-6                    [-1, 2]               4\n",
      "              ReLU-7                    [-1, 2]               0\n",
      "            Linear-8                   [-1, 32]              96\n",
      "      ch_attention-9           [-1, 32, 28, 28]               0\n",
      "           Conv2d-10            [-1, 2, 28, 28]              64\n",
      "      BatchNorm2d-11            [-1, 2, 28, 28]               4\n",
      "             ReLU-12            [-1, 2, 28, 28]               0\n",
      "           Conv2d-13            [-1, 2, 28, 28]              36\n",
      "      BatchNorm2d-14            [-1, 2, 28, 28]               4\n",
      "             ReLU-15            [-1, 2, 28, 28]               0\n",
      "           Conv2d-16            [-1, 2, 28, 28]              36\n",
      "      BatchNorm2d-17            [-1, 2, 28, 28]               4\n",
      "             ReLU-18            [-1, 2, 28, 28]               0\n",
      "           Conv2d-19            [-1, 1, 28, 28]               3\n",
      "spatial_attention-20           [-1, 32, 28, 28]               0\n",
      "              BAM-21           [-1, 32, 28, 28]               0\n",
      "        MaxPool2d-22           [-1, 32, 14, 14]               0\n",
      "           Conv2d-23           [-1, 64, 14, 14]          18,496\n",
      "             ReLU-24           [-1, 64, 14, 14]               0\n",
      "AdaptiveAvgPool2d-25             [-1, 64, 1, 1]               0\n",
      "             View-26                   [-1, 64]               0\n",
      "           Linear-27                    [-1, 4]             256\n",
      "      BatchNorm1d-28                    [-1, 4]               8\n",
      "             ReLU-29                    [-1, 4]               0\n",
      "           Linear-30                   [-1, 64]             320\n",
      "     ch_attention-31           [-1, 64, 14, 14]               0\n",
      "           Conv2d-32            [-1, 4, 14, 14]             256\n",
      "      BatchNorm2d-33            [-1, 4, 14, 14]               8\n",
      "             ReLU-34            [-1, 4, 14, 14]               0\n",
      "           Conv2d-35            [-1, 4, 14, 14]             144\n",
      "      BatchNorm2d-36            [-1, 4, 14, 14]               8\n",
      "             ReLU-37            [-1, 4, 14, 14]               0\n",
      "           Conv2d-38            [-1, 4, 14, 14]             144\n",
      "      BatchNorm2d-39            [-1, 4, 14, 14]               8\n",
      "             ReLU-40            [-1, 4, 14, 14]               0\n",
      "           Conv2d-41            [-1, 1, 14, 14]               5\n",
      "spatial_attention-42           [-1, 64, 14, 14]               0\n",
      "              BAM-43           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-44             [-1, 64, 7, 7]               0\n",
      "             View-45                 [-1, 3136]               0\n",
      "           Linear-46                  [-1, 128]         401,536\n",
      "             ReLU-47                  [-1, 128]               0\n",
      "           Linear-48                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 423,114\n",
      "Trainable params: 423,114\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.70\n",
      "Params size (MB): 1.61\n",
      "Estimated Total Size (MB): 3.32\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ch_attention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_ch, r=16):\n",
    "        super(ch_attention, self).__init__()\n",
    "        layers = []\n",
    "        layers += [nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   View(-1),\n",
    "                   nn.Linear(in_features=n_ch, out_features=n_ch//r, bias=False),\n",
    "                   nn.BatchNorm1d(n_ch//r),\n",
    "                   nn.ReLU(True),\n",
    "                   nn.Linear(in_features=n_ch//r, out_features=n_ch)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    # unsqueeze()함수는 인수로 받은 위치에 새로운 차원을 삽입   \n",
    "    # expand_as(x)로 나머지 차원에다가 인풋 이미지 크기대로 넣어주는 것   \n", 
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).unsqueeze(2).unsqueeze(3).expand_as(x) \n",
    "    \n",
    "class spatial_attention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_ch, r=16, dilation=4):\n",
    "        super(spatial_attention, self).__init__()\n",
    "        layers = []\n",
    "        layers += [nn.Conv2d(in_channels=n_ch, out_channels=n_ch//r, kernel_size=1, bias=False),\n",
    "                   nn.BatchNorm2d(n_ch//r),\n",
    "                   nn.ReLU(True),\n",
    "                   nn.Conv2d(in_channels=n_ch//r, out_channels=n_ch//r, kernel_size=3, padding=dilation, dilation=dilation, bias=False),\n",
    "                   nn.BatchNorm2d(n_ch//r), # 파이토치에서는 dilation 있는만큼 padding을 해줘야 다음에 크기가 안 줄어든다\n",
    "                   nn.ReLU(True),\n",
    "                   nn.Conv2d(in_channels=n_ch//r, out_channels=n_ch//r, kernel_size=3, padding=dilation, dilation=dilation, bias=False),\n",
    "                   nn.BatchNorm2d(n_ch//r),\n",
    "                   nn.ReLU(True),\n",
    "                   nn.Conv2d(in_channels=n_ch//r, out_channels=1, kernel_size=1)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x).expand_as(x)\n",
    "\n",
    "class BAM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_ch, r=16, dilation=4):\n",
    "        super(BAM, self).__init__()\n",
    "        self.channel_att = ch_attention(n_ch, r)\n",
    "        self.spatial_att = spatial_attention(n_ch, r, dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ch_out = self.channel_att(x)\n",
    "        sp_out = self.spatial_att(x)\n",
    "        out = 1 + F.sigmoid(ch_out*sp_out)\n",
    "        return out * x\n",
    "\n",
    "class CNN_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN_Attention, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers += [nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "                   nn.ReLU(inplace=True),\n",
    "                   BAM(32),\n",
    "                   nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                   nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                   nn.ReLU(inplace=True),\n",
    "                   BAM(64),\n",
    "                   nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                   View(-1),\n",
    "                   nn.Linear(in_features=3136, out_features=128),\n",
    "                   nn.ReLU(inplace=True),\n",
    "                   nn.Linear(in_features=128, out_features=10)]\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class View(nn.Module):\n",
    "\n",
    "    def __init__(self, *shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], *self.shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from torchsummary import summary\n",
    "    model = CNN_Attention()\n",
    "    summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 28, 28, 32)   320         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 28, 28, 2)    64          conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 28, 28, 2)    8           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 28, 28, 2)    0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 28, 28, 2)    36          activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 28, 28, 2)    8           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_15 (Gl (None, 32)           0           conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 28, 28, 2)    0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 2)            64          global_average_pooling2d_15[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 28, 28, 2)    36          activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 2)            8           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 28, 28, 2)    8           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 2)            0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 28, 28, 2)    0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 32)           96          activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 28, 28, 1)    3           activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 1, 1, 32)     0           dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_25 (Multiply)          (None, 28, 28, 32)   0           conv2d_75[0][0]                  \n",
      "                                                                 reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 28, 28, 32)   0           multiply_25[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_26 (Multiply)          (None, 28, 28, 32)   0           conv2d_71[0][0]                  \n",
      "                                                                 activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 28, 28, 32)   0           conv2d_71[0][0]                  \n",
      "                                                                 multiply_26[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 14, 14, 32)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 14, 14, 32)   0           max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 14, 14, 64)   18496       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 14, 14, 4)    256         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 14, 14, 4)    16          conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 14, 14, 4)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 14, 14, 4)    144         activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 14, 14, 4)    16          conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_16 (Gl (None, 64)           0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 14, 14, 4)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 4)            256         global_average_pooling2d_16[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 14, 14, 4)    144         activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 4)            16          dense_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 14, 14, 4)    16          conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 4)            0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 14, 14, 4)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_40 (Dense)                (None, 64)           320         activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 14, 14, 1)    5           activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 1, 1, 64)     0           dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_27 (Multiply)          (None, 14, 14, 64)   0           conv2d_80[0][0]                  \n",
      "                                                                 reshape_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 14, 14, 64)   0           multiply_27[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_28 (Multiply)          (None, 14, 14, 64)   0           conv2d_76[0][0]                  \n",
      "                                                                 activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 14, 14, 64)   0           conv2d_76[0][0]                  \n",
      "                                                                 multiply_28[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 7, 7, 64)     0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 3136)         0           max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 128)          401536      flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 10)           1290        dense_41[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 423,162\n",
      "Trainable params: 423,114\n",
      "Non-trainable params: 48\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 케라스\n",
    "\n",
    "from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense, Activation, BatchNormalization, Flatten, Reshape, multiply, add, MaxPooling2D, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "def spatial_attention(in_layer, in_ch, r, d):\n",
    "    x = Conv2D(in_ch//r, kernel_size=(1,1), padding='same', use_bias=False)(in_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(in_ch//r, kernel_size=(3,3), dilation_rate=d, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(in_ch//r, kernel_size=(3,3), dilation_rate=d, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(1, kernel_size=(1,1), padding='same')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def channel_attention(in_layer, in_ch, r):\n",
    "    x = GlobalAveragePooling2D()(in_layer)\n",
    "    # x = Flatten()(x) ?? 원래 케라스에서는 GlobalAvgPooling 하면 자동으로 1차원으로 줄어든다. 그래서 그 담에 Flatten 안 넣어준 것\n",
    "    x = Dense(in_ch//r, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(in_ch)(x)\n",
    "    x = Reshape((1, 1, in_ch))(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def Bottleneck_Attention_Module(in_layer, in_ch, r, d):\n",
    "    Mc = channel_attention(in_layer, in_ch, r)\n",
    "    Ms = spatial_attention(in_layer, in_ch, r, d)\n",
    "    M = multiply([Ms, Mc])\n",
    "    M = Activation('sigmoid')(M)\n",
    "    \n",
    "    x = multiply([in_layer, M])\n",
    "    x = add([in_layer, x])\n",
    "    \n",
    "    return x\n",
    "    \n",
    "r = 16\n",
    "d = 4\n",
    "\n",
    "inputs = Input(shape=(28,28, 1))\n",
    "\n",
    "x = Conv2D(filters=32, kernel_size=(3, 3), strides=(1,1), padding='same', activation='relu')(inputs)\n",
    "x = Bottleneck_Attention_Module(x, 32, r, d)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
    "x = Bottleneck_Attention_Module(x, 64, r, d)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
